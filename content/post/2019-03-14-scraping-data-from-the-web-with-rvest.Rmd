---
title: Scraping Data from the Web with rvest
author: Frederick Solt
date: '2019-03-14'
tags:
  - note
slug: scraping-data
output:
  html_document:
    self_contained: yes
---

Although there are lots of R packages that offer special purpose data-download tools---Eric Persson's [`gesis`](https://cran.r-project.org/web/packages/gesis/vignettes/gesis.html) is one of my favorites, and I'm fond of [`icpsrdata`](https://cran.r-project.org/web/packages/icpsrdata/vignettes/icpsrdata-vignette.html) and [`ropercenter`](https://cran.r-project.org/web/packages/ropercenter/vignettes/ropercenter-vignette.html) too---but the Swiss Army knife of webscraping is the `rvest` package.  That is to say, if there's a specialized tool for the data you're after, you're much better off using _that_ tool, but if not, then `rvest` will help you to get the job done. 

In this post, I'll explain how to do two common webscraping tasks using `rvest`: scraping tables from the web straight into R and scraping the links to a bunch of files so you can then do a batch download.


## Scraping Tables

Scraping data from tables on the web is a simple, three-step process:

1. read the html of the webpage with the table using `read_html()`

2. extract the table using `html_table()`

3. wrangle as needed

So let's suppose we wanted to get the latest [population figures for the countries of Latin America from Wikipedia](http://en.wikipedia.org/wiki/List_of_Latin_American_countries_by_population)

![](/post/2019-03-14-scraping-data-from-the-web-with-rvest_files/wiki_latam.png){width=100%}

### Step 1: Read the Webpage
We load the `tidyverse` and `rvest` packages, then paste the url of the Wikipedia page into `read_html()`

```{r}
library(rvest)
library(lubridate)
library(janitor)
library(tidyverse)


webpage <- read_html("http://en.wikipedia.org/wiki/List_of_Latin_American_countries_by_population")
```


### Step 2: Extract the Table
So far, so good.  Next, we extract the table using `html_table()`.  Because the last row of the table doesn't span all of the columns, we need to use the argument `fill = TRUE` to set the remaining columns to `NA`.  Helpfully, the function will prompt us to do this if we don't recognize we need to.  One wrinkle is that `html_table()` returns a list of all of the tables on the webpage.  In this case, there's only one table, but we still get back a list of length one.  Since what we really want is a dataframe, not a list, we'll use `first()` to grab the first element of the list. If we had a longer list and wanted some middle element, we'd use `nth()`.  And we'll make the dataframe into a tibble for the extra goodness that that format gives us.

```{r}
latam_pop <- webpage %>% 
    html_table(fill=TRUE) %>%   # generates a list of all tables
    first() %>%                 # gets the first element of the list
    as_tibble()                 # makes the dataframe a tibble
    
latam_pop
```

### Step 3: Wrangle as Needed

Nice, clean, tidy data is rare in the wild, and data scraped from the web is definitely wild.  So let's practice our data-wrangling skills here a bit.  The column names of tables are usually not suitable for use as variable names, and our Wikipedia population table is not exceptional in this respect:

```{r}
names(latam_pop)
```

These names are problematic because they have embedded spaces and punctuation marks that may even cause errors when the names are wrapped in backticks, which means we need a way of renaming them without even specifying their names the first time, if you see what I mean.  One solution is the `clean_names()` function of the `janitor` package:^[Remember, if you haven't installed it before, you will need to download `janitor` to your machine with `install.packages("janitor")` before this code will work.]

```{r, message=FALSE}
latam_pop <- latam_pop %>% janitor::clean_names()

names(latam_pop)
```

These names won't win any awards, but they won't cause errors, so `janitor` totally succeeded in cleaning up.  It's a great option for taking care of big messes fast.  At the other end of the scale, if you only need to fixing just one or a few problematic names, you can use `rename()`'s new(-ish) assign-by-position ability:

```{r}
latam_pop <- webpage %>% 
    html_table(fill=TRUE) %>%   # generates a list of all tables
    first() %>%                 # gets the first element of the list
    as_tibble() %>%             # makes the dataframe a tibble
    rename("est_pop_2015" = 3)  # rename the third variable

names(latam_pop)
```

An intermediate option is to assign a complete vector of names (that is, one for every variable):

```{r}
names(latam_pop) <- c("rank", "country", "est_pop_2015",
                      "percent_latam", "annual_growth_rate",
                      "annual_growth", "doubling_time",
                      "official_pop", "date_pop",
                      "source")

names(latam_pop)
```



Okay, enough about variable names; we have other problems here.  For one thing, several of the country/territory names have their colonial powers in parentheses, and there are also Wikipedia-style footnote numbers in brackets that we don't want here either:

```{r}
latam_pop$country[17:27]
```

We can use regex to get rid of that stuff.  In the regex pattern below, recall that the double slashes are "escapes": they mean that we want actual brackets (not a character class) and parentheses (not a capture group).  The `.*`s stand for "anything, repeated zero or more times", and the `|` means "or."  So this `str_replace_all()` is going to replace matched brackets or parentheses and anything between them with nothing.

Finally, while we're at it, we'll also get rid of the line listing the region's total population.

```{r}
latam_pop <- latam_pop %>% 
    mutate(country = str_replace_all(country, "\\[.*\\]|\\(.*\\)", "") %>% 
               str_trim()) %>% 
    filter(country!="Total")

latam_pop$country[17:nrow(latam_pop)]
```

Another common problem with webscraped tables: are the numbers encoded as strings?  Probably so.

```{r}
latam_pop$official_pop
str(latam_pop$official_pop)
```

Yep.  So let's replace the commas with nothing and use `as.numeric()` to make the result actually numeric.

```{r}
latam_pop <- latam_pop %>% 
    mutate(official_pop = str_replace_all(official_pop, ",", "") %>%
               as.numeric())

latam_pop$official_pop
str(latam_pop$official_pop)
```

One last issue is to get the dates for these population figures into POSIXct format.  This is complicated a bit by the fact for some countries we only have a year rather than a full date:

```{r}
latam_pop$date_pop
```

We deal with this using `if_else()` and `str_detect()` to find which dates begin (`^`) with a digit (`\\d`), and then let the `lubridate` package's `parse_date_time()` function know that those are years and the rest are in "month day, year" format.

```{r, warning=FALSE}
latam_pop <- latam_pop %>% 
    mutate(date_pop = if_else(str_detect(date_pop, "^\\d"),
                              lubridate::parse_date_time(date_pop,
                                                         "y"),
                              lubridate::parse_date_time(date_pop,
                                                         "m d, y")))

latam_pop$date_pop
```

And we're done!  One point worth considering is whether you want this particular scrape to be reproducible.  Sometime you won't---maybe the page you're scraping is updated regularly, and you always want the latest data.  But if you do want to be sure to be able to reproduce what you've done in the future, I recommend you take a second, go to [the Internet Archive](https://archive.org), [archive the page](https://archive.org/web/), and then _scrape the archived page_ instead of the live one.

![](/post/2019-03-14-scraping-data-from-the-web-with-rvest_files/Screen Shot 2019-03-14 at 6.34.54 PM.png){width=100%}



## Scraping Files

It's also pretty common to want to get a bunch of files linked from a website.  Consider this example from my own work on [the Standardized World Income Inequality Database](https://fsolt.org/swiid/).  The SWIID depends on [source data on income inequality](https://fsolt.org/swiid/swiid_source/) from international organizations and national statistical offices around the world, including Armenia's Statistical Committee (ArmStat).  ArmStat has a annual report on food security and poverty that includes a chapter titled "Accessibility of Food" that has some data on income and consumption inequality that I want.

Here's the page with the list of poverty reports:
![](/post/2019-03-14-scraping-data-from-the-web-with-rvest_files/sg0.png){width=100%}

Clicking on the top link shows a list of chapters.  Note the chapter on "Accessibility of Food":
![](/post/2019-03-14-scraping-data-from-the-web-with-rvest_files/Screen Shot 2019-03-14 at 12.20.19 PM.png){width=100%}

Clicking on that pdf icon shows the pdf file of the chapter, where we can see, if we scroll down a bit, the table with the lovely data:
![](/post/2019-03-14-scraping-data-from-the-web-with-rvest_files/Screen Shot 2019-03-13 at 1.51.38 PM.png){width=100%}

But again, we want to download that file for that chapter for every one of the annual reports listed on that first page.  We _could_ go carefully down the list of all the reports, pick out the annual reports, click through the chapters over and over, but there's more than a dozen annual reports here, and we're still at the start of the alphabet as far as countries go.  Further, there'll be _another_ new annual report next year.  We need to have this process automated.

Thinking about it a moment, this is maybe a particularly tricky scraping job because we need to:

1. get the links for the reports we want from the main page on poverty, and then

2. follow each of those links to get the link for the chapter we want.  

3. After we have all _those_ links, then we can download all the chapter files and extract the data we want from them.  (Reading pdfs into R is another great data acquisition skill, but I think it merits a post of its own, so for now we'll stop once we have all the files we want.)

In a more straightforward scraping job, step 2 wouldn't be necessary at all, but this sort of arrangement isn't all that uncommon (think of articles in issues of journals, for another example), and it will give us a chance to practice some of our data wrangling skills.


### Step 1: Getting the Links to the Reports

Like with scraping web tables, we will start by pasting our link into `read_html()`, but now we will need some more information about just which part of the page we want to scrape, information we'll get from a neat little tool called SelectorGadget.  The `rvest` package includes [a nice vignette on how to install and use it here](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html).  With SelectorGadget installed in our bookmarks bar (called the favorites bar in Safari here) and engaged (just click on it), each element of the page gets boxed in color as you mouse over it:

![](/post/2019-03-14-scraping-data-from-the-web-with-rvest_files/sg1.png){width=100%}

The idea is to get everything you want---and nothing you don't want---highlighted in green or yellow.  Clicking on the first link gives us all the links of the reports . . . but also all the other links:

![](/post/2019-03-14-scraping-data-from-the-web-with-rvest_files/sg2.png){width=100%}

So we click on one of the links we _don't_ want to turn it red.  In this case, this actually leaves us with exactly what we want, just the links of the reports:

![](/post/2019-03-14-scraping-data-from-the-web-with-rvest_files/sg3.png){width=100%}

Note that in the wide SelectorGadget box at the bottom of the window, it says "h4 a"---that's the info we'll use to identify the parts of the webpage we want, using `rvest`'s `html_nodes()` function.  This will result in a list of xml nodes.  We'll make a tibble of these nodes, with one variable for the title of the report and one for its link.  The report title is just the link text, which we get by passing the node to `html_text()`; for consistency's sake (some of the titles of the early reports are in all caps), we'll make all of the titles lowercase with `str_to_lower()`. To get the report link url, we use `html_attr("href")`.  In this case, the urls are expressed relative to the ArmStat homepage, so we also need to swap out the dot which stands for that address with the actual address.  We only want the year-end reports, so we'll use the combination of `filter()` and `str_detect()` to keep only those that include "december" in their title, and the titles of some reports indicate that they are only available in Armenian, so we'll filter those out, too.

```{r}
library(tidyverse)
library(rvest)

reports_nodes <- read_html("http://www.armstat.am/en/?nid=81&pthid=pov&year=") %>% 
  html_nodes("h4 a")
  
reports_links <- tibble(report_title = reports_nodes %>%
                                html_text() %>%
                                str_to_lower(),
                            report_link = reports_nodes %>% 
                                html_attr("href") %>% 
                                str_replace("\\.",
                                            "https://www.armstat.am/en")) %>% 
    filter(str_detect(report_title, "december") & 
               !str_detect(report_title, "armenian"))

reports_links
```


### Step 2: Getting the Links to the Chapter Files

Now that we have the links to all of the reports, we need to get the link to the chapter file we want from each one of them.  We'll start with the first report and then use the `tidyverse`'s iteration tools, `map` and `walk`, to apply our code to all of the reports.

Right, so let's take on that first report.  Back to SelectorGadget!  Clicking on the link for the first report, engaging SelectorGadget, and clicking on the chapter title "Accessibility of food" gives us the "td" tag, which defines a cell in an html table.  This means that each of our chapter nodes will be a separate cell from this web table, and we'll have to reconstruct the table structure ourselves.  If we knew a bit of html, we could use the "tr" tag, which defines rows, instead, and spare ourselves this fancy footwork, but whenever I want to know anything about html I have to look it up.  You may be in the same boat, so let's just plan on doing the footwork.

![](/post/2019-03-14-scraping-data-from-the-web-with-rvest_files/Screen Shot 2019-03-14 at 12.49.01 PM.png){width=100%}

Again, we'll make a tibble of titles and links from our chapter nodes.  Again, we'll use `html_text()` to get the titles and standardize on lowercase letters.  However, this time, the titles are not themselves links---the links are in subsequent cells in the same row in the table that contain no text at all.  We'll use `if_else()` to test if the title is an empty string, and if so, set it to `NA_character_` (the missing data indicator for strings).  We can get the links, meanwhile, by selecting `html_node("a")`---"a" for anchor defines links in html (we didn't need to do this separately when we were getting the report links above because our tag, "h4 a", already included it)---and then `html_attr("href")` as before.  And we'll have to deal with the relative urls again, too (two dots to replace this time, and no `/en`---we figure this out by clicking the link in the actual webpage and comparing it to what `html_attr("href")` gave us).  How do we deal with the fact the rows of our tibble are just cells from the webtable, so our titles have no links and our links have no titles?  Here comes that "fancy footwork": we'll use the `fill()` function to replace any `NA`s in our chapter titles with the last observed chapter title (this is why we set the empty strings to `NA_character_` the way we did above; `fill()` only works on `NA`s, not empty strings).  Then, since each link is actually repeated twice---ArmStat understandably wants to be sure you know that their bilingual documents are available to you in both Armenian and English---we use `distinct()` to get rid of the duplicates.  Finally, we drop the observations without links (that is, the original title cells), leaving us with a neat reconstruction of the table of chapter titles and links:

```{r}
report_link <- reports_links %>% 
    pull(report_link) %>% 
    first()

chapter_nodes <- read_html(report_link) %>% 
    html_nodes("td")

chapter_links <- tibble(chapter_title = chapter_nodes %>% 
                            html_text() %>% 
                            str_to_lower() %>% 
                            if_else(. == "", NA_character_, .),
                        chapter_link = chapter_nodes %>% 
                            html_node("a") %>% 
                            html_attr("href") %>% 
                            str_replace("\\.\\.",
                                        "https://www.armstat.am/en")) %>% 
    fill(chapter_title) %>% 
    distinct() %>% 
    filter(!is.na(chapter_link))
    
chapter_links
```

Of course, we just want the "accessibility of food" chapter, and, in fact, really just the link rather than a tibble:

```{r}
accessibilty_chapter <- chapter_links %>% 
    filter(str_detect(chapter_title, "accessibility"))

accessibilty_chapter

just_the_link <- accessibilty_chapter %>% 
        pull(chapter_link)

just_the_link
```

All right, so now we're going to take everything we just did to get the link for the first report's accessibility chapter and wrap it into a function that takes a report link and returns `just_the_link` for the accessibilty chapter:

```{r}
get_chapter_link <- function(report_link) {
    chapter_nodes <- read_html(report_link) %>% 
        html_nodes("td")
    
    chapter_links <- tibble(chapter_title = chapter_nodes %>% 
                               html_text() %>% 
                               str_to_lower() %>% 
                               if_else(. == "", NA_character_, .),
                           chapter_link = chapter_nodes %>% 
                               html_node("a") %>% 
                               html_attr("href") %>% 
                               str_replace("\\.\\.", "https://www.armstat.am/")) %>% 
        fill(chapter_title) %>% 
        distinct() %>% 
        filter(!is.na(chapter_link))
    
    accessibilty_chapter <- chapter_links %>% 
        filter(str_detect(chapter_title, "accessibility"))

    just_the_link <- accessibilty_chapter %>% 
        pull(chapter_link)

    return(just_the_link)
}

```

Now we can use that function to create a new chapter link variable in our `reports_links` tibble, with the wrinkle that `mutate()` expects a vectorized function---that is, one that operates on all the values of the variable at the same time---and [we can meet this expectation by using `map_chr()`](https://www.jimhester.com/2018/04/12/vectorize/):


```{r}
reports_links <- reports_links %>% 
    mutate(chapter_link = map_chr(report_link, get_chapter_link))

reports_links
```

## Step 3: Getting! All! the! Files!

This is actually straightforward, now that we have a tibble of report titles and links to chapter files.  The `download.file()` function will save the contents of a link (its first argument) to a filepath (its second argument) and the `walk2()` function allows you to pass a pair of vectors to a function to iteratively use as its two arguments (first the first element of each vector, then the second element of each vector, and so on). We'll use `str_extract()` to get the year of the report from its title, and use it to make a file name for the pdf with `str_c()`.  But first we should create a directory to save those files in.

```{r, eval=FALSE}

dir.create("armstat_files")

walk2(reports_links$report_title, reports_links$chapter_link,
      function(report_title, chapter_link) {
    pdf_path <- file.path("armstat_files",
                          str_c("armstat", 
                                str_extract(report_title, "\\d{4}"),
                                ".pdf"))
    download.file(chapter_link, pdf_path)
})

```

And that's that.  And remember, use your new powers for good, not for evil: be sure to respect the terms of service of any website you might want to scrape.


