---
title: Scraping Data from the Web with rvest
author: Frederick Solt
date: '2019-03-12'
tags:
  - note
slug: scraping-data
draft: yes
output:
  html_document:
    self_contained: yes
---

Although there are lots of R packages that offer special purpose data-download tools---Eric Persson's [`gesis`](https://cran.r-project.org/web/packages/gesis/vignettes/gesis.html) is one of my favorites, and I'm fond of [`icpsrdata`](https://cran.r-project.org/web/packages/icpsrdata/vignettes/icpsrdata-vignette.html) and [`ropercenter`](https://cran.r-project.org/web/packages/ropercenter/vignettes/ropercenter-vignette.html) too---but the Swiss Army knife of webscraping is the `rvest` package.  That is to say, if there's a specialized tool for the data you're after, you're much better off using _that_ tool, but if not, then `rvest` will help you to get the job done. 

In this post, I'll explain how to do two common webscraping tasks using `rvest`: scraping tables from the web straight into R and scraping the links to a bunch of files so you can then do a batch download.

## Scraping Tables

Scraping data from tables on the web is a simple, three-step process:

1. read the html of the webpage with the table using `read_html()`

2. extract the table using `html_table()`

3. wrangle as needed

So let's suppose we wanted to get the latest [population figures for the countries of Latin America from Wikipedia](http://en.wikipedia.org/wiki/List_of_Latin_American_countries_by_population)

![](/post/2019-03-12-scraping-data-from-the-web-with-rvest_files/wiki_latam.png){width=80%}

### Step 1: Read the Webpage
We load the `tidyverse` and `rvest` packages, then paste the url of the Wikipedia page into `read_html()`

```{r}
library(tidyverse)
library(rvest)

webpage <- read_html("http://en.wikipedia.org/wiki/List_of_Latin_American_countries_by_population")
```


### Step 2: Extract the Table
So far, so good.  Next, we extract the table using `html_table()`.  Because the last row of the table doesn't span all of the columns, we need to use the argument `fill = TRUE` to set the remaining columns to `NA`.  Helpfully, the function will prompt us to do this if we don't recognize we need to.  One wrinkle is that `html_table()` returns a list of all of the tables on the webpage.  In this case, there's only one table, but we still get back a list of length one.  Since what we really want is a dataframe, not a list, we'll use `first()` to grab the first element of the list. If we had a longer list and wanted some middle element, we'd use `nth()`.  And we'll make the dataframe into a tibble for the extra goodness that that format gives us.

```{r}
latam_pop <- webpage %>% 
    html_table(fill=TRUE) %>%   # generates a list of all tables
    first() %>%                 # gets the first element of the list
    as_tibble()                 # makes the dataframe a tibble
    
latam_pop
```

### Step 3: Wrangle as Needed

Nice, clean, tidy data is rare in the wild, and data scraped from the web is definitely wild.  So let's practice our data-wrangling skills here a bit.  The column names of tables are usually not suitable for use as variable names, and our Wikipedia population table is not exceptional in this respect:

```{r}
names(latam_pop)
```

These names are problematic because they have embedded spaces and punctuation marks that may even cause errors when the names are wrapped in backticks, which means we need a way of renaming them without even specifying their names the first time, if you see what I mean.  One solution is the `clean_names()` function of the `janitor` package:^[Remember, if you haven't installed it before, you will need to download `janitor` to your machine with `install.packages("janitor")` before this code will work.]

```{r, message=FALSE}
latam_pop <- latam_pop %>% janitor::clean_names()

names(latam_pop)
```

These names won't win any awards, but they won't cause errors, so `janitor` totally succeeded in cleaning up.  It's a great option for taking care of big messes fast.  At the other end of the scale, if you only need to fixing just one or a few problematic names, you can use `rename()`'s new(-ish) assign-by-position ability:

```{r}
latam_pop <- webpage %>% 
    html_table(fill=TRUE) %>%   # generates a list of all tables
    first() %>%                 # gets the first element of the list
    as_tibble() %>%             # makes the dataframe a tibble
    rename("est_pop_2015" = 3)  # rename the third variable

names(latam_pop)
```

An intermediate option is to assign a complete vector of names (that is, one for every variable):

```{r}
names(latam_pop) <- c("rank", "country", "est_pop_2015",
                      "percent_latam", "annual_growth_rate",
                      "annual_growth", "doubling_time",
                      "official_pop", "date_pop",
                      "source")

names(latam_pop)
```



Okay, enough about variable names; we have other problems here.  For one thing, several of the country/territory names have their colonial powers in parentheses, and there are also Wikipedia-style footnote numbers in brackets that we don't want here either:

```{r}
latam_pop$country[17:27]
```

We can use regex to get rid of that stuff.  In the regex pattern below, recall that the double slashes are "escapes": they mean that we want actual brackets (not a character class) and parentheses (not a capture group).  The `.*`s stand for "anything, repeated zero or more times", and the `|` means "or."  So this `str_replace_all()` is going to replace matched brackets or parentheses and anything between them with nothing.

Finally, while we're at it, we'll also get rid of the line listing the region's total population.

```{r}
latam_pop <- latam_pop %>% 
    mutate(country = str_replace_all(country, "\\[.*\\]|\\(.*\\)", "") %>% 
               str_trim()) %>% 
    filter(country!="Total")

latam_pop$country[17:nrow(latam_pop)]
```

Another common problem with webscraped tables: are the numbers encoded as strings?  Probably so.

```{r}
latam_pop$official_pop
str(latam_pop$official_pop)
```

Yep.  So let's replace the commas with nothing and use `as.numeric()` to make the result actually numeric.

```{r}
latam_pop <- latam_pop %>% 
    mutate(official_pop = str_replace_all(official_pop, ",", "") %>%
               as.numeric())

latam_pop$official_pop
str(latam_pop$official_pop)
```

One last issue is to get the dates for these population figures into POSIXct format.  This is complicated a bit by the fact for some countries we only have a year rather than a full date:

```{r}
latam_pop$date_pop
```

We deal with this using `if_else()` and `str_detect()` to find which dates begin (`^`) with a digit (`\\d`), and then let the `lubridate` package's `parse_date_time()` function know that those are years and the rest are in "month day, year" format.

```{r, warning=FALSE}
latam_pop <- latam_pop %>% 
    mutate(date_pop = if_else(str_detect(date_pop, "^\\d"),
                              lubridate::parse_date_time(date_pop, "y"),
                              lubridate::parse_date_time(date_pop, "m d, y")))

latam_pop$date_pop
```

And we're done!

## Scraping Files

It's also pretty common to want to get a bunch of files linked from a website.  Consider this example from my own work on [the Standardized World Income Inequality Database](https://fsolt.org/swiid/).  The SWIID depends on [source data on income inequality](https://fsolt.org/swiid/swiid_source/) from international organizations and national statistical offices around the world, including Armenia's Statistical Committee (ArmStat).  ArmStat has a regular report on food security and poverty that includes a chapter titled "Accessibility of Food" that has some data on income and consumption inequality that I want.

Here's the page with the list of poverty reports:
![](/post/2019-03-12-scraping-data-from-the-web-with-rvest_files/sg0.png){width=80%}

Clicking on the top link shows a list of chapters.  Note the chapter on "Accessibility of Food":
![](/post/2019-03-12-scraping-data-from-the-web-with-rvest_files/Screen Shot 2019-03-14 at 12.20.19 PM.png){width=80%}

Clicking on that pdf icon shows the pdf file of the chapter, with the lovely data:
![](/post/2019-03-12-scraping-data-from-the-web-with-rvest_files/Screen Shot 2019-03-13 at 1.51.38 PM.png){width=80%}

But again, we want to download that file for that chapter for every one of the dozens of reports listed on that first page.  We _could_ click through over and over, but there's a _lot_ of files here (87 at the moment), and we're still at the start of the alphabet as far as countries go.  We need to automate this process.

Thinking about it a moment, this is maybe a particularly tricky scrape because we need to:

1. get the links for the reports we want from the main page on poverty, and then

2. follow each of those links to get the link for the chapter we want.  

3. After we have all _those_ links, then we can download all the chapter files and extract the data we want from them.  

Reading pdfs into R is another great data acquisition skill, but I think it merits a post of its own, so for now we'll stop once we have all the files we want.

### Step 1: Getting the Links to the Reports

Like with scraping web tables, we will start by pasting our link into `read_html()`, but now we will need some more information about just which part of the page we want to scrape, information we'll get from a neat little tool called SelectorGadget.  The `rvest` package includes [a nice vignette on how to install and use it here](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html).  With SelectorGadget installed in our bookmarks bar (called the favorites bar in Safari here) and engaged (just click on it), each element of the page gets boxed in color as you mouse over it:

![](/post/2019-03-12-scraping-data-from-the-web-with-rvest_files/sg1.png){width=80%}

The idea is to get everything you want---and nothing you don't want---highlighted in green or yellow.  Clicking on the first link gives us all the links of the reports . . . but also all the other links:

![](/post/2019-03-12-scraping-data-from-the-web-with-rvest_files/sg2.png){width=80%}

So we click on one of the links we _don't_ want to turn it red.  In this case, this actually leaves us with exactly what we want, just the links of the reports:

![](/post/2019-03-12-scraping-data-from-the-web-with-rvest_files/sg3.png){width=80%}

Note that in the wide SelectorGadget box at the bottom of the window, it says "h4 a"---that's the info we'll use to identify the parts of the webpage we want, using `rvest`'s `html_nodes()` function.  This will result in a list of xml nodes.  We'll make a tibble of these nodes, with one variable for the title of the report and one for its link.  The report title is just the link text, which we get by passing the node to `html_text()`; for consistency's sake (some of the titles of the early reports are in all caps), we'll make all of the titles lowercase with `str_to_lower()`. To get the link url, we use `html_attr("href")`.  In this case, the links are expressed relative to the ArmStat homepage, so we need to swap out the dot which stands for that address with the actual address.

```{r}
library(tidyverse)
library(rvest)

arm_reports_nodes <- read_html("http://www.armstat.am/en/?nid=81&pthid=pov&year=") %>% 
  html_nodes("h4 a")
  
arm_reports_links <- tibble(report_title = arm_reports_nodes %>%
                                html_text() %>%
                                str_to_lower(),
                            report_link = arm_reports_nodes %>% 
                                html_attr("href") %>% 
                                str_replace("\\.", "https://www.armstat.am/en"))

arm_reports_links
```

### Step 2: Getting the Links to the Chapter Files


```{r}
get_access_link <- function(link) {
  access <- map(link, function(link) {
    t <- read_html(link) %>% 
      html_nodes("tr") 
    t2 <- t[str_detect(t %>% html_text(), "Accessibility|ACCESSIBILITY")] %>% 
      html_node("a") %>% 
      html_attr("href") %>% 
      str_replace("\\.\\.", "http://www.armstat.am") 
    return(t2)
  }) %>% 
    unlist()
  return(access)
}



# chapters_page <- read_html(link)

# chapter_links <- tibble(chapter_title = chapters_page %>% 
#     html_nodes("td") %>% 
#     html_text(),
#     chapter_link = chapters_page %>% 
#     html_nodes("td") %>%
#         html_node("a") %>%  # a tags define links
#         html_attr("href"))
#     
# 
# arm_reports <- tibble(title = arm_page %>% html_text() %>% tolower(),
#                       link1 = arm_page %>% html_attr("href")) %>% 
#   filter(str_detect(title, "december") &        # We want reports that cover the whole year
#              !str_detect(title, "armenian"))    # We don't want the reports written in Armenian
# 
# # %>% 
# #   mutate(link1 = str_replace(link1, "\\.", "http://www.armstat.am/en"),
# #          link2 = get_access_link(link1),
# #          year = str_extract(title, "\\d{4}"))
# 
# get_arm_ginis <- function(link) {
#   file_yr <- str_extract(link, "\\d{2,4}") %>% 
#     if_else(str_length(.) == 2, str_c("20", .), .)
#   file_name <- paste0("data-raw/armstat", file_yr, ".pdf")
#   if (!file.exists(file_name)) download.file(link, file_name)
#   ginis <- extract_tables(file_name, pages = 11)[[1]] %>% 
#     as_tibble()
#   if (!any(str_detect(ginis$V1, "Gini"))) {
#     ginis <- extract_tables(file_name, pages = 12)[[1]]
#   }
#   ginis1 <- ginis %>% 
#     first_row_to_names() %>% 
#     mutate(v1 = replace(v1, v1=="", NA_character_)) %>% 
#     fill(v1, .direction = "up") %>% 
#     filter(str_detect(v1, "inequality")) %>% 
#     gather(key = year, value = gini, -v1) %>% 
#     filter(!gini=="") %>% 
#     mutate(report = as.numeric(file_yr),
#            link = link)
#   return(ginis1)
# }

```

